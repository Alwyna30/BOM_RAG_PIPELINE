# -*- coding: utf-8 -*-
"""Rag_Pipeline_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NZDdTQFIbB5fqimpI0_u5XI_xz522-mM

**Creating Chunks**
"""

!pip install langchain tiktoken pandas

import pandas as pd

df = pd.read_csv("/content/bom_loan_clean.csv")

print(df.head(2))

from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators = ["\n", ".", "!", "?", ":", ";"]
)

chunks=[]

for _, row in df.iterrows():
  content = str(row["filtered_content"])
  loan_type = row["loan_type"]

  for chunk in splitter.split_text(content):
        chunks.append({
            "loan_type": loan_type,
            "chunk": chunk.strip()
        })

chunks_df = pd.DataFrame(chunks)
print("Chunking complete. Total chunks:", len(chunks_df))
chunks_df.head(3)

import os

os.makedirs("data/processed", exist_ok=True)
output_path = "data/processed/bom_loan_chunks.csv"
chunks_df.to_csv(output_path, index=False)
print(f"File saved at {output_path}")

"""**Creating Emmbeddings**"""

df = pd.read_csv("data/processed/bom_loan_chunks.csv")
print(df.head())
print(f"Total chunks: {len(df)}")

from sentence_transformers import SentenceTransformer

model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
df["embedding"] = df["chunk"].apply(lambda x: model.encode(x).tolist())

os.makedirs("data/vector_data", exist_ok=True)
df.to_pickle("data/vector_data/bom_with_embeddings.pkl")
print("Embeddings saved successfully")

"""**Storing In Vector Database - ChromaDB**"""

!pip install chromadb --quiet

import chromadb
from chromadb.utils import embedding_functions
import pandas as pd

client = chromadb.PersistentClient(path="data/chroma_db")

embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

df = pd.read_pickle("data/vector_data/bom_with_embeddings.pkl")

collection = client.get_or_create_collection(
    name="bom_loan_docs",
    embedding_function=embedding_func
)

collection.add(
    ids=[str(i) for i in range(len(df))],
    documents=df["chunk"].tolist(),
    metadatas=[{"loan_type":t} for t in df["loan_type"].tolist()],
)

print("All data added to chromadb")

query = "What is the interest rate for education loans?"
results = collection.query(query_texts=[query], n_results=8)

for i, doc in enumerate(results["documents"][0]):
    print(f"\nResult {i+1}:")
    print(doc[:400])

"""**Integrating LightWeight LLM**"""

!pip install -q huggingface_hub
!huggingface-cli login

from huggingface_hub import InferenceClient


hf_token = os.environ.get("HUGGINGFACE_HUB_API_TOKEN")


client = InferenceClient(
    model="mistralai/Mistral-7B-Instruct-v0.2",
    token=hf_token
)

context = "\n".join(results["documents"][0])
query = input("Enter your question\n")

prompt = f"""
You are a banking assistant for Bank of Maharashtra.
Based on the context below, answer the question accurately.
Only answer using the context provided, otherwise respond that information isnâ€™t available.

Context:
{context}

Question: {query}

"""

response = client.chat_completion(
    messages=[{"role": "user", "content": prompt}],
    max_tokens=300,
    temperature=0.3
)

print("\nFinal Answer:")
print(response.choices[0].message["content"])

